# EEG2Video

Transform electroencephalographic (EEG) activity into coherent video sequences using stateâ€‘ofâ€‘theâ€‘art deepâ€‘learning techniques: Conformers, Transformerâ€‘encoders, 3â€‘D UNets with crossâ€‘attention, and diffusion generative models. The repository also contains classical EEGâ€‘classification baselines and extensive preprocessing utilities.

---

## ðŸ“‘ TableÂ ofÂ Contents

1. [Project Goals](#project-goals)
2. [Directory Layout](#directory-layout)
3. [Main Workflows](#main-workflows)
4. [Key APIsÂ &Â Scripts](#key-apis--scripts)
5. [Installation](#installation)
6. [Quick Start](#quick-start)
7. [Evaluation](#evaluation)
8. [Notes & Roadmap](#notes--roadmap)

---

## Project Goals

* **EEGâ€‘toâ€‘Video**Â â€“ learn a mapping from multichannel EEG sequences to short video clips.
* **EEGâ€‘VP baselines**Â â€“ benchmark shallow/deep CNNs, Conformers and MLPs on motorâ€‘imagery classification.
* **Modular Pipeline**Â â€“ decouple preprocessing, feature engineering, model training, and evaluation for reproducibility.

---

## Directory Layout

```text
EEG2Video/
â”‚
â”œâ”€â”€ analyse_tree.py                # Staticâ€‘analysis helper (lists functions/classes)
â”‚
â”œâ”€â”€ EEG-VP/                        # EEGâ€¯classification baselines
â”‚   â”œâ”€â”€ EEG_VP_train_test.py       # Training / test loop & helpers
â”‚   â””â”€â”€ models.py                  # ShallowNet, DeepNet, EEGNet, Conformer, â€¦
â”‚
â”œâ”€â”€ Gaspard_preprocess/            # Personal preprocessing utilities
â”‚   â”œâ”€â”€ import.py                  # Load & plot raw blocks
â”‚   â”œâ”€â”€ yaml_gen.py                # YAML metadata generator
â”‚   â”œâ”€â”€ process_video.py           # Extract 2â€¯s clips, downâ€‘sample videos
â”‚   â””â”€â”€ plot_data.py               # Quick visual checks
â”‚
â”œâ”€â”€ Gaspard_model/                 # Training scripts & custom models
â”‚   â”œâ”€â”€ train_glmnet_cv.py         # Crossâ€‘validated GLMNet trainer
â”‚   â”œâ”€â”€ train_model_comparison.py  # ShallowNetâ€¯vsâ€¯Deep baselines
â”‚   â”œâ”€â”€ train_shallownet_{cv,paper}.py # ShallowNet experiments
â”‚   â”œâ”€â”€ train_mlp_cv.py            # MLP on PSD/DE features
â”‚   â””â”€â”€ models/                    # Encoders, Transformers, etc.
â”‚       â”œâ”€â”€ encoders.py            # CLIP, GLMNetEncoder, MLPEncoder, â€¦
â”‚       â”œâ”€â”€ transformers.py        # EEG2VideoTransformer
â”‚       â””â”€â”€ models.py              # Video & EEG backbones (shared)
â”‚
â”œâ”€â”€ EEG_preprocessing/             # Signal segmentation & feature extraction
â”‚   â”œâ”€â”€ segment_raw_signals_200Hz.py
â”‚   â”œâ”€â”€ segment_sliding_window.py
â”‚   â”œâ”€â”€ DE_PSD.py                  # Differential Entropy & Power Spectral Density
â”‚   â”œâ”€â”€ extract_DE_PSD_features_{1per1s,1per2s}.py
â”‚   â””â”€â”€ gen_features_from_sw_data.py
â”‚
â”œâ”€â”€ EEG2Video/                     # Core EEGâ€‘toâ€‘Video pipeline
â”‚   â”œâ”€â”€ inference_eeg2video.py     # Zeroâ€‘shot / fineâ€‘tuned inference
â”‚   â”œâ”€â”€ train_finetune_videodiffusion.py
â”‚   â”œâ”€â”€ 40_class_run_metrics.py    # MSE,Â SSIM,Â CLIP,Â Topâ€‘k metrics
â”‚   â”œâ”€â”€ models/                    # Diffusersâ€‘style latent models
â”‚   â”œâ”€â”€ pipelines/                 # Tuneâ€‘aâ€‘Video and EEGâ€‘conditioned versions
â”‚   â””â”€â”€ models/â€¦ (resnet, unet, attention, â€¦)
â”‚
â””â”€â”€ EEG2Video_New/                 # Experimental v2 pipeline (modularised)
    â””â”€â”€ â€¦ (mirrors the structure above)
```

> **â„¹ï¸Ž Tip:** duplicate model definitions in `models/models.py` are kept for backward compatibility and will be consolidated in a future refactor.

---

## Main Workflows

| Stage | Script / Entryâ€‘point | Description |
|-------|----------------------|-------------|
| **1. Preâ€‘processing** | `EEG_preprocessing/segment_raw_signals_200Hz.py`<br>`EEG_preprocessing/extract_DE_PSD_features_*.py` | Slice raw `.npy` recordings into windows (200â€¯Hz) and compute DE/PSD features. |
| **2. Feature Engineering** | `EEG_preprocessing/gen_features_from_sw_data.py` | Aggregate slidingâ€‘window features for downstream tasks. |
| **3. EEG Baselines** | `EEG-VP/EEG_VP_train_test.py` | Train ShallowNet / EEGNet / Conformer baselines on classification. |
| **4. GLMNet & MLP** | `Gaspard_model/train_glmnet_cv.py`<br>`Gaspard_model/train_mlp_cv.py` | Crossâ€‘validated training on spectral features. |
| **5. EEGâ€‘toâ€‘Video** | `EEG2Video/train_finetune_videodiffusion.py` | Fineâ€‘tune latentâ€‘diffusion pipeline conditioned on EEG embeddings. |
| **6. Inference** | `EEG2Video/inference_eeg2video.py` | Generate video clips from unseen EEG segments. |
| **7. Evaluation** | `EEG2Video/40_class_run_metrics.py` | Compute clip/video accuracy, CLIP Score, MSE, SSIM, PSNR, etc. |

---

## Key APIsÂ &Â Scripts

Below is a nonâ€‘exhaustive registry of public classes & utilities (autoâ€‘generated via `analyse_tree.py`). Use it as a quick reference when importing:

### Core Helpers

- **`analyse_tree.py`** â€“ `list_functions_and_classes`, `scan_project`
- **`Gaspard_preprocess/import.py`** â€“ `load_all_eeg_data_by_subject`, `plot_eeg_block`

### Representative Models

| Path | Classes |
|------|---------|
| `Gaspard_model/models/encoders.py` | `CLIP`, `GLMNetEncoder`, `MLPEncoder`, `ShallowNetEncoder`, `MLPEncoder_feat` |
| `Gaspard_model/models/transformers.py` | `EEG2VideoTransformer` |
| `EEG2Video/models/unet.py` | `UNet3DConditionModel`, `UNet3DConditionOutput` |
| `EEG2Video/models/DANA_module.py` | `Diffusion` |

*(Expand the full list with `analyse_tree.py` when developing new components.)*

---

## Installation

```bash
# 1. Clone
$ git clone https://github.com/yourâ€‘username/EEG2Video.git
$ cd EEG2Video

# 2. Environment
$ python -m venv .venv
$ source .venv/bin/activate  # Windows: .venv\Scripts\activate
$ pip install -r requirements.txt
```

CUDAÂ 11.8Â +â€¯PyTorchâ€¯2.2 are recommended for 3â€‘D diffusion training.

---

## Quick Start

```bash
# Finetune diffusion on preprocessed EEG
python EEG2Video/train_finetune_videodiffusion.py \
       --config configs/finetune.yaml

# Generate video from a saved EEG feature file
python EEG2Video/inference_eeg2video.py \
       --eeg ./samples/example.npy --output ./out/
```

---

## Evaluation

Run the comprehensive metrics suite:

```bash
python EEG2Video/40_class_run_metrics.py \
       --pred_dir ./out/ --gt_dir ./ground_truth/ \
       --metrics clip mse ssim topk
```

Outputs include perâ€‘video JSON logs and an aggregated CSV summary.

---

## Notes &Â Roadmap

- [ ] **Model consolidation** â€“ unify duplicate `models/models.py` across subâ€‘packages.
- [ ] **Lightning migration** â€“ port training scripts to PyTorchÂ Lightning for cleaner checkpoints.
- [ ] **Web demo** â€“ stream generated clips via Gradio.

Contributions via pull requests or issues are welcome! Feel free to open a discussion for feature requests or questions.

